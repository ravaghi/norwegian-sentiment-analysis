{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = os.path.abspath(os.path.join('..'))\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "os.environ[\"WANDB_SILENT\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightning.pytorch.callbacks.early_stopping import EarlyStopping\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "import lightning as L\n",
    "import wandb\n",
    "\n",
    "from data.dataloader import NoReCDataLoader\n",
    "from data.preprocessor import NoReCDataPreprocessor\n",
    "from dataloaders.lstm import NoReCDataModule\n",
    "from models.lstm import LSTM\n",
    "from utils.utils import init_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = init_run(config_name=\"multiclass_lstm\", run_name=\"Multiclass-LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df, test_df = NoReCDataLoader(**config.dataloader).load_multiclass_dataset()\n",
    "\n",
    "preprocessor = NoReCDataPreprocessor()\n",
    "\n",
    "train_df = preprocessor.sanitize(train_df, \"train\")\n",
    "val_df = preprocessor.sanitize(val_df, \"val\")\n",
    "test_df = preprocessor.sanitize(test_df, \"test\")\n",
    "\n",
    "vocab, tokenizer = preprocessor.build_vocabulary(train_df, config.data.vocab_size)\n",
    "\n",
    "train_df = preprocessor.tokenize(train_df, vocab, tokenizer)\n",
    "val_df = preprocessor.tokenize(val_df, vocab, tokenizer)\n",
    "test_df = preprocessor.tokenize(test_df, vocab, tokenizer)\n",
    "\n",
    "train_df = preprocessor.pad(train_df, vocab, config.data.max_seq_len)\n",
    "val_df = preprocessor.pad(val_df, vocab, config.data.max_seq_len)\n",
    "test_df = preprocessor.pad(test_df, vocab, config.data.max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = NoReCDataModule(\n",
    "    train_df=train_df,\n",
    "    val_df=val_df,\n",
    "    test_df=test_df,\n",
    "    batch_size=config.general.batch_size\n",
    ")\n",
    "class_weights = data_module.get_class_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(**config.model, n_class=3, class_weights=class_weights)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_auc\", patience=3, mode=\"max\", verbose=True, check_on_train_epoch_end=True)\n",
    "trainer = L.Trainer(\n",
    "    max_epochs=config.general.max_epochs,\n",
    "    logger=WandbLogger(save_dir=config.general.log_dir),\n",
    "    callbacks=[early_stopping],\n",
    ")\n",
    "\n",
    "trainer.fit(model, data_module)\n",
    "trainer.validate(model, data_module)\n",
    "trainer.test(model, data_module)\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
